# BackPropagation_from_scratch
Implementing Backpropagation with multiple optimization algorithms from scratch using only numpy for calculations
to run the project the command is **python train.py** you can pass arguments such as which optimizer to use learning rate and etc.
You can check for the parameters that you can pass from the terminal by **python train.py -h**
For new functionality you can edit the code file named **feedForward.py**

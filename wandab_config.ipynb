{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tqdm import tqdm\n",
    "import decimal\n",
    "import tensorflow as tf\n",
    "import wandb as wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z,-500,500)\n",
    "    return 1.0 / (1 + np.exp(-(z)))\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "# def relu(z):\n",
    "#     return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
    "#     #return np.maximum(z,0)\n",
    "#     #return np.where(z<0, 0.01*z, z)\n",
    "def relu(z):\n",
    "    return np.maximum(z*0.01,z)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    ep = 1e-5\n",
    "    Z = np.clip(Z,-600,600)\n",
    "    return (np.exp(Z) / (np.sum(np.exp(Z)))+ep)\n",
    "\n",
    "# def softmax(Z):\n",
    "#     c = Z.max()\n",
    "#     logsumexp = np.log(np.exp(Z - c).sum())\n",
    "#     return Z - c - logsumexp\n",
    "\n",
    "\n",
    "def grad_sigmoid(z):\n",
    "    return  sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def grad_tanh(z):\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "\n",
    "# def grad_relu(z):\n",
    "#     return (z>0)*np.ones(z.shape) + (z<0)*(0.01*np.ones(z.shape) )\n",
    "\n",
    "def grad_relu(x):\n",
    "    alpha=0.01\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x < 0] = alpha\n",
    "    return dx\n",
    "\n",
    "## sizeHL is a number it can be converted back to list\n",
    "class FeedForwardNN:\n",
    "    def __init__(\n",
    "        self, layers, sizeHL, x_train, y_train, x_val, y_val, \n",
    "        x_test, y_test, batchSize, lr, iterations, activation,\n",
    "        initializer, loss, weight_decay):\n",
    "        self.classes = 10\n",
    "        self.layers = layers+2\n",
    "        self.layersSize = [784] + [sizeHL for i in range(layers)] + [10]\n",
    "        self.total = x_train.shape[0]\n",
    "        # 784 * 60000\n",
    "        self.X = np.transpose(x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]))\n",
    "        self.X_test = np.transpose(x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\n",
    "        self.X_val = np.transpose(x_val.reshape(x_val.shape[0],x_val.shape[1]*x_val.shape[2]))\n",
    "        #normalizing values\n",
    "        self.X = self.X/255\n",
    "        self.X_test = self.X_test/255\n",
    "        self.X_val = self.X_val/255\n",
    "        # changing normal values to one hot encoding\n",
    "        # shape will be 10*60000\n",
    "        \n",
    "        self.Y = self.oneHotEncoder(y_train)\n",
    "        self.Y_test = self.oneHotEncoder(y_test)\n",
    "        self.Y_val = self.oneHotEncoder(y_val)\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        self.y_test = y_test\n",
    "        self.Activations_dict = {\"sigmoid\": sigmoid, \"tanh\": tanh, \"relu\": relu}\n",
    "        self.derActivation_dict = {\n",
    "            \"sigmoid\": grad_sigmoid,\n",
    "            \"tanh\": grad_tanh,\n",
    "            \"relu\": grad_relu,\n",
    "        }\n",
    "\n",
    "        self.Initializer_dict = {\n",
    "            \"xavier\": self.xavier,\n",
    "            \"random\": self.random,\n",
    "        }\n",
    "\n",
    "        # self.Optimizer_dict = {\n",
    "        #     \"SGD\": self.sgd,\n",
    "        #     \"MGD\": self.mgd,\n",
    "        #     \"NAG\": self.nag,\n",
    "        #     \"RMSPROP\": self.rmsProp,\n",
    "        #     \"ADAM\": self.adam,\n",
    "        #     \"NADAM\": self.nadam,\n",
    "        # }\n",
    "        self.activation = self.Activations_dict[activation]\n",
    "        self.derivation_activation = self.derActivation_dict[activation]\n",
    "    #    self.optimzer = self.Optimizer_dict[optimizer]\n",
    "        self.initializer = self.Initializer_dict[initializer]\n",
    "        self.lossFunction = loss\n",
    "        self.weightDecay = weight_decay\n",
    "        self.epochs = iterations\n",
    "        self.batchSize = batchSize\n",
    "        self.lr = lr\n",
    "\n",
    "        # initializing weights and biases\n",
    "        self.weights, self.biases = self.initializeWeights(self.layersSize)\n",
    "\n",
    "\n",
    "    def train(self,optimizer,momentum=0.9,beta1=0.9,beta2=0.999):\n",
    "        if(optimizer=='sgd'):\n",
    "            self.sgd()\n",
    "        elif(optimizer=='mgd'):\n",
    "            self.mgd(momentum)\n",
    "        elif(optimizer=='nag'):\n",
    "            self.nag(momentum)\n",
    "        elif(optimizer=='rmsprop'):\n",
    "            self.rmsProp(momentum)\n",
    "        elif(optimizer=='adam'):\n",
    "            self.adam(beta1,beta2)\n",
    "        elif(optimizer=='nadam'):\n",
    "            self.nadam(beta1,beta2)\n",
    "\n",
    "\n",
    "    def oneHotEncoder(self,rawY):\n",
    "        onehot = np.zeros((self.classes,rawY.shape[0]))\n",
    "        size = rawY.shape[0]\n",
    "        for i in range(size):\n",
    "            onehot[int(rawY[i])][i] = 1.0\n",
    "        return onehot\n",
    "    \n",
    "\n",
    "\n",
    "    def accuracyLoss(self,x,y):\n",
    "        pred,H,A = self.forwardPropagation(x)\n",
    "        pred = pred.T\n",
    "        count =0\n",
    "        crossloss = []\n",
    "        for i in range(len(y)):\n",
    "            if(np.argmax(pred[i])==y[i]):\n",
    "                count+=1\n",
    "        if self.loss == \"cross_entropy\":\n",
    "            loss  = -np.mean(np.sum(self.oneHotEncoder(y).T * np.log(pred + 1e-15), axis=1))\n",
    "            return count/(len(y)),loss\n",
    "        elif self.loss == \"mean_squared_error\":\n",
    "            loss = self.meanSquaredErrorLoss(self.oneHotEncoder(y).T,pred)\n",
    "            return count/len(y) , loss\n",
    "    def meanSquaredErrorLoss( y, pred):\n",
    "        mse = np.mean((y - pred) ** 2)\n",
    "        return mse\n",
    "\n",
    "    def crossEntropyLoss( y, pred):\n",
    "        CE = [-Y_true[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
    "        crossEntropy = np.mean(CE)\n",
    "        return crossEntropy\n",
    "\n",
    "    def initializeWeights(self,layersSize):\n",
    "        weights =[]\n",
    "        biases =[]\n",
    "        for i in range(len(layersSize)-1):\n",
    "            weights.append(self.initializer([layersSize[i+1],layersSize[i]]))\n",
    "            biases.append(self.initializer([layersSize[i+1],1]))\n",
    "        return weights,biases\n",
    "\n",
    "    def xavier(self, size):\n",
    "        std = np.sqrt(2 / (size[0] + size[1]))\n",
    "        # size[0] = next layer's neurons\n",
    "        # size[1] = prev layer's neurons\n",
    "        return np.random.normal(0, std, size=(size[0], size[1]))\n",
    "\n",
    "    def random(self, size):\n",
    "        return np.random.normal(0, 1, size=(size[0], size[1]))\n",
    "\n",
    "    def forwardPropagation(self,X):\n",
    "        layers = len(self.weights)\n",
    "        H =[0 for i in range(layers-1)]\n",
    "        A =[0 for i in range(layers)]\n",
    "        for i in range(layers-1):\n",
    "            if i==0:\n",
    "                A[i] = np.add(np.dot(self.weights[i],X),self.biases[i])\n",
    "                H[i] = self.activation(A[i])\n",
    "            else:\n",
    "                A[i] = np.add(np.dot(self.weights[i],H[i-1]),self.biases[i])\n",
    "                H[i] = self.activation(A[i])\n",
    "        A[layers-1] = np.add(np.dot(self.weights[layers-1],H[layers-2]),self.biases[layers-1])\n",
    "        #print(A[layers-1])\n",
    "        pred = softmax(A[layers-1])\n",
    "        return pred,H,A\n",
    "\n",
    "    def backPropagation(self, pred, weights, H, A, x_train, y_train):\n",
    "        dW = []\n",
    "        dB = []\n",
    "        # x_train has the shape of 784*batch size\n",
    "        #output layer gradient\n",
    "        #if self.lossFunction =='CROSS':\n",
    "        gradients =[]\n",
    "        l = len(self.layersSize)-2\n",
    "        # print(l)\n",
    "\n",
    "        if self.lossFunction =='cross_entropy':\n",
    "            # gradients['a'+str(l)] = -(y_train - pred)\n",
    "            gradients.append(-(y_train - pred))\n",
    "        elif self.lossFunction=='mean_squared_error':\n",
    "            # gradients['a'+str(l)] = np.multiply(2*(pred-y_train),np.multiply(pred,(1-pred))) \n",
    "            gradients.append(np.multiply(2*(pred-y_train),np.multiply(pred,(1-pred))) )\n",
    "        #print(weights[0])\n",
    "        # print(l)\n",
    "        for i in range(l,0,-1):\n",
    "            # dw = np.dot(gradients['a'+str(i)],H[i-1].T)\n",
    "            dw = np.dot(gradients[0],H[i-1].T)\n",
    "            # db = np.sum(gradients['a'+str(i)],axis=1).reshape(-1,1)\n",
    "            db = np.sum(gradients[0],axis=1).reshape(-1,1)\n",
    "            dW.append(dw)\n",
    "            dB.append(db)\n",
    "            # print(\"iteration : {i}\")\n",
    "            # print(self.weights[i].shape)\n",
    "            # print(gradients['a'+str(i)].shape)\n",
    "            \n",
    "            # dh = np.matmul(weights[i].T,gradients['a'+str(i)])\n",
    "            dh = np.matmul(weights[i].T,gradients[0])\n",
    "            gradients[0] = np.multiply(dh,self.derivation_activation(A[i-1]))\n",
    "        dW.append(np.dot(gradients[0],x_train.T))\n",
    "        dB.append(np.sum(gradients[0],axis =1).reshape(-1,1))\n",
    "        dW.reverse()\n",
    "        dB.reverse()\n",
    "\n",
    "        for i in range(self.layers-1):\n",
    "            dW[i] = dW[i] + self.weights[i]*self.weightDecay\n",
    "        return dW,dB\n",
    "    \n",
    "    def sgd(self,weight_decay=0):\n",
    "        iterations = self.epochs\n",
    "        layers = self.layers\n",
    "\n",
    "        totalData = self.X.shape[-1]\n",
    "        for i in tqdm(range(iterations)):\n",
    "            j =0\n",
    "            dW=[]\n",
    "            dB=[]\n",
    "            while(j<totalData):\n",
    "                pred,H,A  = self.forwardPropagation(self.X[:,j:j+self.batchSize])\n",
    "                dW,dB = self.backPropagation(pred, self.weights, H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
    "                # 10,batch size = pred.shape\n",
    "                #print(pred.shape)\n",
    "                j+=self.batchSize\n",
    "\n",
    "                for k in range(layers-1):\n",
    "                    self.weights[k] = self.weights[k] - self.lr*dW[k]\n",
    "                    self.biases[k] = self.biases[k] - self.lr*dB[k]\n",
    "            train_acc, train_loss = self.accuracyLoss(self.X,self.y_train)\n",
    "            val_acc, val_loss = self.accuracyLoss(self.X_val,self.y_val)\n",
    "            #print(type(train_acc),type(train_loss),type(val_acc),type(val_loss))\n",
    "            print(\"Train Accuracy - %.5f, Val Accuracy - %.5f, Train Loss - %.5f, Val Loss - %.5f,  EPOCH ==> %d\"%(train_acc,val_acc,train_loss,val_loss,i+1))\n",
    "            wb.log({\"Train_Accuracy\":train_acc,\"Val_Accuracy\":val_acc,\"Train_Loss\":train_loss,\"Val_Loss\":val_loss,\"epoch\":i})\n",
    "            if((train_acc>0.09000 and train_acc<=0.10000) or (val_acc>0.09000 and val_acc<=0.01000) ):\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    def mgd(self,beta):\n",
    "        #print(self.derivation_activation)\n",
    "        iterations = self.epochs\n",
    "        layers = self.layers\n",
    "        uW = [0 for i in range(layers-1)]\n",
    "        uB = [0 for i in range(layers-1)]\n",
    "        totalsamples = self.X.shape[-1]\n",
    "        for i in tqdm(range(iterations)):\n",
    "            j =0\n",
    "            dW =[]\n",
    "            dB =[]\n",
    "            while(j<totalsamples):\n",
    "                pred, H, A = self.forwardPropagation(self.X[:,j:j+self.batchSize])\n",
    "                dW, dB = self.backPropagation(pred, self.weights, H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
    "                j+=self.batchSize\n",
    "\n",
    "                for k in range(layers-1):\n",
    "                    uW[k] = uW[k]*beta + dW[k]\n",
    "                    uB[k] = uB[k]*beta + dB[k]\n",
    "                \n",
    "                for k in range(layers-1):\n",
    "                    self.weights[k] -= self.lr*uW[k]\n",
    "                    self.biases[k] -= self.lr*uB[k]\n",
    "\n",
    "            train_acc, train_loss = self.accuracyLoss(self.X,self.y_train)\n",
    "            val_acc, val_loss = self.accuracyLoss(self.X_val,self.y_val)\n",
    "            #print(type(train_acc),type(train_loss),type(val_acc),type(val_loss))\n",
    "            print(\"Train Accuracy - %.5f, Val Accuracy - %.5f, Train Loss - %.5f, Val Loss - %.5f,  EPOCH ==> %d\"%(train_acc,val_acc,train_loss,val_loss,i+1))\n",
    "            wb.log({\"Train_Accuracy\":train_acc,\"Val_Accuracy\":val_acc,\"Train_Loss\":train_loss,\"Val_Loss\":val_loss,\"epoch\":i})\n",
    "            if((train_acc>0.09000 and train_acc<=0.10000) or (val_acc>0.09000 and val_acc<=0.01000) ):\n",
    "                break    \n",
    "\n",
    "\n",
    "\n",
    "    def nag(self,beta):\n",
    "        iterations = self.epochs\n",
    "        layers = self.layers\n",
    "        print(layers)\n",
    "        vW = [0 for i in range(layers-1)]\n",
    "        vB = [0 for i in range(layers-1)]\n",
    "        pvW = [0 for i in range(layers-1)]\n",
    "        pvB = [0 for i in range(layers-1)]\n",
    "        v_W = [0 for i in range(layers-1)]\n",
    "        v_B = [0 for i in range(layers-1)]\n",
    "        totalpoints = self.X.shape[-1]\n",
    "        for i in tqdm(range(iterations)):\n",
    "            j =0\n",
    "            dW =[]\n",
    "            dB =[]\n",
    "            for k in range(layers-1):\n",
    "                v_W[k] = beta*pvW[k]\n",
    "                v_B[k] = beta*pvB[k]\n",
    "            while(j<totalpoints):\n",
    "                nw=[]\n",
    "                pred, H, A = self.forwardPropagation(self.X[:,j:j+self.batchSize])\n",
    "                for p in range(layers-1):\n",
    "                    nw.append(self.weights[p] - v_W[p])\n",
    "                dW,dB = self.backPropagation(pred, nw, H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
    "\n",
    "                for l in range(layers-1):\n",
    "                    vW[l] = beta*pvW[l] + self.lr*dW[l]\n",
    "                    self.weights[l] -= vW[l]\n",
    "                    vB[l] = beta*pvB[l] + self.lr*dB[l]\n",
    "                    self.biases[l] -= vB[l]\n",
    "                pvW = vW\n",
    "                pvB = vB\n",
    "                j+=self.batchSize\n",
    "            train_acc, train_loss = self.accuracyLoss(self.X,self.y_train)\n",
    "            val_acc, val_loss = self.accuracyLoss(self.X_val,self.y_val)\n",
    "            #print(type(train_acc),type(train_loss),type(val_acc),type(val_loss))\n",
    "            print(\"Train Accuracy - %.5f, Val Accuracy - %.5f, Train Loss - %.5f, Val Loss - %.5f,  EPOCH ==> %d\"%(train_acc,val_acc,train_loss,val_loss,i+1))\n",
    "            wb.log({\"Train_Accuracy\":train_acc,\"Val_Accuracy\":val_acc,\"Train_Loss\":train_loss,\"Val_Loss\":val_loss,\"epoch\":i})\n",
    "            if((train_acc>0.09000 and train_acc<=0.10000) or (val_acc>0.09000 and val_acc<=0.01000) ):\n",
    "                break\n",
    "\n",
    "\n",
    "    def rmsProp(self,beta):\n",
    "        \n",
    "        layers = self.layers                    \n",
    "        print(layers)\n",
    "        vW = [0 for i in range(layers-1)]\n",
    "        vB = [0 for i in range(layers-1)]\n",
    "        eps =1e-4\n",
    "        totalsamples = self.X.shape[-1]\n",
    "        for i in tqdm(range(self.epochs)):\n",
    "            j =0\n",
    "            while(j<totalsamples):\n",
    "                pred, H, A = self.forwardPropagation(self.X[:,j:j+self.batchSize])\n",
    "                dW, dB = self.backPropagation(pred, self.weights,H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
    "\n",
    "\n",
    "                for k in range(layers-1):\n",
    "                    vW[k] = beta*vW[k] + (1-beta)*(dW[k]**2)\n",
    "                    vB[k] = beta*vB[k] + (1-beta)*(dB[k]**2)\n",
    "                for k in range(layers-1):\n",
    "                    self.weights[k] -= self.lr*(dW[k]/(np.sqrt(vW[k])+eps))\n",
    "                    self.biases[k] -= self.lr*(dB[k]/(np.sqrt(vB[k])+eps))\n",
    "                \n",
    "                j+=self.batchSize\n",
    "            \n",
    "            train_acc, train_loss = self.accuracyLoss(self.X,self.y_train)\n",
    "            val_acc, val_loss = self.accuracyLoss(self.X_val,self.y_val)\n",
    "            #print(type(train_acc),type(train_loss),type(val_acc),type(val_loss))\n",
    "            print(\"Train Accuracy - %.5f, Val Accuracy - %.5f, Train Loss - %.5f, Val Loss - %.5f,  EPOCH ==> %d\"%(train_acc,val_acc,train_loss,val_loss,i+1))\n",
    "            wb.log({\"Train_Accuracy\":train_acc,\"Val_Accuracy\":val_acc,\"Train_Loss\":train_loss,\"Val_Loss\":val_loss,\"epoch\":i})\n",
    "            if((train_acc>0.09000 and train_acc<=0.10000) or (val_acc>0.09000 and val_acc<=0.01000) ):\n",
    "                break            \n",
    "\n",
    "\n",
    "    def adam(self,beta1,beta2):\n",
    "        layers = self.layers\n",
    "        m_w,m_b,v_w,v_b = [[0 for l in range(layers-1)] for k in range(4)]\n",
    "        totalsamples = self.X.shape[-1]\n",
    "        eps = 1e-10\n",
    "        for i in range(self.epochs):\n",
    "            j =0\n",
    "\n",
    "            while(j<totalsamples):\n",
    "                pred, H, A = self.forwardPropagation(self.X[:,j:j+self.batchSize])\n",
    "                dW, dB = self.backPropagation(pred, self.weights, H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
    "\n",
    "                j+=self.batchSize\n",
    "\n",
    "                for p in range(layers-1):\n",
    "                    m_w[p] = beta1*m_w[p] + (1-beta1)*dW[p]\n",
    "                    m_b[p] = beta1*m_b[p] + (1-beta1)*dB[p]\n",
    "                    v_w[p] = beta2*v_w[p] + (1-beta2)*(dW[p]**2)\n",
    "                    v_b[p] = beta2*v_b[p] + (1-beta2)*(dB[p]**2)\n",
    "                m_w_hat,m_b_hat,v_b_hat,v_w_hat = [[0 for c in range(layers-1)] for q in range(4)]\n",
    "                for c in range(layers-1):\n",
    "                    m_w_hat[c] = m_w[c]/(1-np.power(beta1,i+1))\n",
    "                    m_b_hat[c] = m_b[c]/(1-np.power(beta1,i+1))\n",
    "                    v_w_hat[c] = v_w[c]/(1-np.power(beta2,i+1))\n",
    "                    v_b_hat[c] = v_b[c]/(1-np.power(beta2,i+1))\n",
    "\n",
    "                for c in range(layers-1):\n",
    "                    self.weights[c] -= self.lr*m_w_hat[c]/(np.sqrt(v_w_hat[c])+eps)\n",
    "                    self.biases[c] -= self.lr*m_b_hat[c]/(np.sqrt(v_b_hat[c])+eps)\n",
    "            \n",
    "            train_acc, train_loss = self.accuracyLoss(self.X,self.y_train)\n",
    "            val_acc, val_loss = self.accuracyLoss(self.X_val,self.y_val)\n",
    "            #print(type(train_acc),type(train_loss),type(val_acc),type(val_loss))\n",
    "            print(\"Train Accuracy - %.5f, Val Accuracy - %.5f, Train Loss - %.5f, Val Loss - %.5f,  EPOCH ==> %d\"%(train_acc,val_acc,train_loss,val_loss,i+1))\n",
    "            wb.log({\"Train_Accuracy\":train_acc,\"Val_Accuracy\":val_acc,\"Train_Loss\":train_loss,\"Val_Loss\":val_loss,\"epoch\":i})\n",
    "            if((train_acc>0.09000 and train_acc<=0.10000) or (val_acc>0.09000 and val_acc<=0.01000) ):\n",
    "                break\n",
    "\n",
    "\n",
    "    def nadam(self,beta1,beta2):\n",
    "        layers = self.layers\n",
    "        m_w,m_b,v_w,v_b = [[0 for i in range(layers-1)] for k in range(4)]\n",
    "        eps = 1e-10\n",
    "        totalsamples = self.X.shape[-1]\n",
    "        for i in range(self.epochs):\n",
    "            j = 0\n",
    "            while(j<totalsamples):\n",
    "                pred, H, A = self.forwardPropagation(self.X[:,j:j+self.batchSize])\n",
    "                dW, dB = self.backPropagation(pred, self.weights, H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
    "                j+=self.batchSize\n",
    "\n",
    "                for p in range(layers-1):            \n",
    "                    m_w[p] = beta1*m_w[p]+(1-beta1)*dW[p]\n",
    "                    m_b[p] = beta1*m_b[p]+(1-beta1)*dB[p]\n",
    "                    v_w[p] = beta2*v_w[p]+(1-beta2)*dW[p]**2\n",
    "                    v_b[p] = beta2*v_b[p]+(1-beta2)*dB[p]**2\n",
    "\n",
    "                m_w_hat,m_b_hat,v_b_hat,v_w_hat = [[0 for c in range(layers-1)] for q in range(4)]\n",
    "                for c in range(layers-1):\n",
    "                    m_w_hat[c] = m_w[c]/(1-np.power(beta1,i+1))\n",
    "                    m_b_hat[c] = m_b[c]/(1-np.power(beta1,i+1))\n",
    "                    v_w_hat[c] = v_w[c]/(1-np.power(beta2,i+1))\n",
    "                    v_b_hat[c] = v_b[c]/(1-np.power(beta2,i+1))\n",
    "                    \n",
    "                for c in range(layers-1):\n",
    "                    self.weights[c] -= (self.lr/np.sqrt(v_w_hat[c]+eps))*(beta1*m_w_hat[c]+(1-beta1)*dW[c]/(1-beta1**(i+1)))\n",
    "                    self.biases[c] -= (self.lr/np.sqrt(v_b_hat[c]+eps))*(beta1*m_b_hat[c]+(1- beta1)*dB[c]/(1-beta1**(i+1)))\n",
    "                \n",
    "            \n",
    "            train_acc, train_loss = self.accuracyLoss(self.X,self.y_train)\n",
    "            val_acc, val_loss = self.accuracyLoss(self.X_val,self.y_val)\n",
    "            #print(type(train_acc),type(train_loss),type(val_acc),type(val_loss))\n",
    "            print(\"Train Accuracy - %.5f, Val Accuracy - %.5f, Train Loss - %.5f, Val Loss - %.5f,  EPOCH ==> %d\"%(train_acc,val_acc,train_loss,val_loss,i+1))\n",
    "            wb.log({\"Train_Accuracy\":train_acc,\"Val_Accuracy\":val_acc,\"Train_Loss\":train_loss,\"Val_Loss\":val_loss,\"epoch\":i})\n",
    "            if((train_acc>0.09000 and train_acc<=0.10000) or (val_acc>0.09000 and val_acc<=0.10000) ):\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    wb.init(project=\"CS6910 - Assignment 1\")\n",
    "    config=wb.config\n",
    "    name = \"opt_\"+str(config.optimizers)+\"_init_\"+str(config.initialization)+\"_layers_\"+str(config.number_of_hidden_layers)+\"_sizeOfHL_\"+str(config.neurons_in_hidden_layers)+\"_lr_\"+str(config.learning_rate)+\"_activations_\"+str(config.activations)+\"_batch_\"+str(config.batch_size)\n",
    "    wb.run.name = name\n",
    "    (train_x,train_y),(test_x,test_y) = fashion_mnist.load_data()\n",
    "    classes =['Ankle boot','T-shirt/top','Dress','Pullover','sneaker','Sandal','Trouser','Shirt','Coat','Bag']\n",
    "    split = 0.1\n",
    "    total_data = train_x.shape[0]\n",
    "    indices = np.arange(total_data)\n",
    "    np.random.shuffle(indices)\n",
    "    train_x = train_x[indices]\n",
    "    train_y = train_y[indices]\n",
    "    data_train = int((1-split)*total_data)\n",
    "    x_train = train_x[:data_train]\n",
    "    y_train = train_y[:data_train]\n",
    "    x_val = train_x[data_train:]\n",
    "    y_val = train_y[data_train:]  \n",
    "    # layers, sizeHL, x_train, y_train, x_val, y_val, x_test, y_test, optimizer, batchSize, lr, iterations, activation,initializer, loss, weightDecay\n",
    "    ob=FeedForwardNN(layers=config.number_of_hidden_layers,sizeHL=config.neurons_in_hidden_layers,x_train=x_train,y_train=y_train,x_val=x_val,y_val=y_val,x_test=test_x,y_test=test_y,batchSize=config.batch_size,lr=config.learning_rate,iterations=config.epochs,activation=config.activations,initializer=config.initialization,loss='cross',weight_decay=config.weight_decay)\n",
    "    ob.train(optimizer=config.optimizers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: nycx7bp7\n",
      "Sweep URL: https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/sweeps/nycx7bp7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lafliv9a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivations: tanh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_value: 0.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: xavier\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: cross_entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tneurons_in_hidden_layers: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_hidden_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizers: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\mtech\\sem2\\deep\\assignment_1\\BackPropagation_from_scratch\\wandb\\run-20240317_230416-lafliv9a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/lafliv9a' target=\"_blank\">hearty-sweep-1</a></strong> to <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/sweeps/nycx7bp7' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/sweeps/nycx7bp7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/sweeps/nycx7bp7' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/sweeps/nycx7bp7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/lafliv9a' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/lafliv9a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8536e2e2454f269978a79cb454d4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded\\r'), FloatProgress(value=0.23130967393262475, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-sweep-1</strong> at: <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/lafliv9a' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/lafliv9a</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240317_230416-lafliv9a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run lafliv9a errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\1985867546.py\", line 21, in main\n",
      "    ob.train(optimizer=config.optimizers)\n",
      "  File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 117, in train\n",
      "    self.nadam(beta1,beta2)\n",
      "  File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 412, in nadam\n",
      "    dW, dB = self.backPropagation(pred, self.weights, H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 205, in backPropagation\n",
      "    dw = np.dot(gradients[0],H[i-1].T)\n",
      "                ~~~~~~~~~^^^\n",
      "IndexError: list index out of range\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run lafliv9a errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Python311\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\1985867546.py\", line 21, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ob.train(optimizer=config.optimizers)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 117, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.nadam(beta1,beta2)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 412, in nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     dW, dB = self.backPropagation(pred, self.weights, H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 205, in backPropagation\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     dw = np.dot(gradients[0],H[i-1].T)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                 ~~~~~~~~~^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m IndexError: list index out of range\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8o6twnd8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivations: tanh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta_value: 0.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: xavier\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: cross_entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tneurons_in_hidden_layers: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_hidden_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizers: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\mtech\\sem2\\deep\\assignment_1\\BackPropagation_from_scratch\\wandb\\run-20240317_230435-8o6twnd8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/8o6twnd8' target=\"_blank\">grateful-sweep-2</a></strong> to <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/sweeps/nycx7bp7' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/sweeps/nycx7bp7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/sweeps/nycx7bp7' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/sweeps/nycx7bp7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/8o6twnd8' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/8o6twnd8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c4d6e001a24f0d9db4c78cd2677755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded\\r'), FloatProgress(value=0.23130967393262475, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sweep-2</strong> at: <a href='https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/8o6twnd8' target=\"_blank\">https://wandb.ai/deeplearning-assignment/CS6910%20-%20Assignment%201/runs/8o6twnd8</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240317_230435-8o6twnd8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 8o6twnd8 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\1985867546.py\", line 21, in main\n",
      "    ob.train(optimizer=config.optimizers)\n",
      "  File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 117, in train\n",
      "    self.nadam(beta1,beta2)\n",
      "  File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 412, in nadam\n",
      "    dW, dB = self.backPropagation(pred, self.weights, H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 205, in backPropagation\n",
      "    dw = np.dot(gradients[0],H[i-1].T)\n",
      "                ~~~~~~~~~^^^\n",
      "IndexError: list index out of range\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 8o6twnd8 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Python311\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\1985867546.py\", line 21, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ob.train(optimizer=config.optimizers)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 117, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.nadam(beta1,beta2)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 412, in nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     dW, dB = self.backPropagation(pred, self.weights, H, A, self.X[:,j:j+self.batchSize],self.Y[:,j:j+self.batchSize])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_10816\\3684192061.py\", line 205, in backPropagation\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     dw = np.dot(gradients[0],H[i-1].T)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                 ~~~~~~~~~^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m IndexError: list index out of range\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'cross and mse new',\n",
    "    'metric': {\n",
    "        'goal': 'maximize',\n",
    "        'name': 'validation_accuracy'\n",
    "        },\n",
    "    'parameters': {\n",
    "\n",
    "        'initialization': {'values': ['xavier']},\n",
    "        'number_of_hidden_layers' : {'values' : [3]},\n",
    "        'neurons_in_hidden_layers' : {'values' : [128]},\n",
    "\n",
    "        'learning_rate': {'values':[1e-4]},\n",
    "        'beta_value' : {'values' : [0.9,0.999]},\n",
    "        'optimizers' : {'values' : ['nadam']},\n",
    "\n",
    "        'batch_size': {'values': [32]},\n",
    "        'epochs': {'values': [15]},\n",
    "        'loss_type' : {'values' : [\"cross_entropy\",\"mean_squared_error\"]},\n",
    "        'activations' : {'values' : ['tanh']},\n",
    "        'weight_decay' : {'values' : [0]}\n",
    "       }\n",
    "    }\n",
    "\n",
    "sweep_id = wb.sweep(sweep=sweep_configuration,project='CS6910 - Assignment 1')\n",
    "\n",
    "wb.agent(sweep_id , function = main , count = 2)\n",
    "wb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 28, 28) (54000,)\n",
      "(6000, 28, 28) (6000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "layers = 3\n",
    "no_of_hidden_neurons = 128\n",
    "sizeHL = [no_of_hidden_neurons for i in range(4)]\n",
    "optimizer = 'nadam'\n",
    "batchSize = 32\n",
    "weight_decay = 0\n",
    "lr = 0.0001\n",
    "iterations = 15\n",
    "activation = 'tanh'\n",
    "initializer = 'xavier'\n",
    "loss= 'cross'\n",
    "momentum = 0.9\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "(train_x,train_y),(test_x,test_y) = fashion_mnist.load_data()\n",
    "classes =['Ankle boot','T-shirt/top','Dress','Pullover','sneaker','Sandal','Trouser','Shirt','Coat','Bag']\n",
    "split = 0.1\n",
    "total_data = train_x.shape[0]\n",
    "indices = np.arange(total_data)\n",
    "np.random.shuffle(indices)\n",
    "train_x = train_x[indices]\n",
    "train_y = train_y[indices]\n",
    "data_train = int((1-split)*total_data)\n",
    "x_train = train_x[:data_train]\n",
    "y_train = train_y[:data_train]\n",
    "x_val = train_x[data_train:]\n",
    "y_val = train_y[data_train:]  \n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy - 0.69126, Val Accuracy - 0.68217, Train Loss - 10.80250, Val Loss - 9.56508,  EPOCH ==> 1\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ob \u001b[38;5;241m=\u001b[39m FeedForwardNN(layers,sizeHL,x_train,y_train,x_val,y_val,test_x,test_y,batchSize,lr,iterations,activation,initializer,loss, weight_decay)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnadam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 429\u001b[0m, in \u001b[0;36mFeedForwardNN.nadam\u001b[1;34m(self, beta1, beta2)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m#print(type(train_acc),type(train_loss),type(val_acc),type(val_loss))\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy - \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m, Val Accuracy - \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m, Train Loss - \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m, Val Loss - \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m,  EPOCH ==> \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(train_acc,val_acc,train_loss,val_loss,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 429\u001b[0m \u001b[43mwb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain_Accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtrain_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVal_Accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mval_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain_Loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVal_Loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m((train_acc\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.09000\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m train_acc\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.10000\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (val_acc\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.09000\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m val_acc\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.10000\u001b[39m) ):\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\wandb\\sdk\\lib\\preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "\n",
    "ob = FeedForwardNN(layers,sizeHL,x_train,y_train,x_val,y_val,test_x,test_y,batchSize,lr,iterations,activation,initializer,loss, weight_decay)\n",
    "ob.nadam(beta1,beta2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred, H, A = ob.feedForward(self.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
